# VPipe å˜é‡é€ŸæŸ¥è¡¨

## ğŸ”– å¿«é€Ÿç´¢å¼•

### StageRuntime ç±»æ ¸å¿ƒå˜é‡ (runtime.py)

#### åŸºç¡€æ ‡è¯†ä¿¡æ¯
| å˜é‡å | ç±»å‹ | å«ä¹‰ | ç¤ºä¾‹å€¼ |
|--------|------|------|--------|
| `self.rank` | int | å…¨å±€è¿›ç¨‹ID | 0-7 (8 GPUs) |
| `self.local_rank` | int | èŠ‚ç‚¹å†…GPU ID | 0-3 |
| `self.stage` | int | å½“å‰stageç¼–å· | 0, 1, 2, 3 |
| `self.rank_in_stage` | int | åœ¨stageå†…çš„rank | 0, 1 (æ•°æ®å¹¶è¡Œ) |
| `self.num_ranks` | int | æ€»è¿›ç¨‹æ•° | 8 |
| `self.num_stages` | int | æ€»stageæ•° | 4 |
| `self.num_ranks_in_stage` | int | å½“å‰stageå†…GPUæ•° | 2 (æ•°æ®å¹¶è¡Œåº¦) |

#### æµæ°´çº¿æ§åˆ¶
| å˜é‡å | ç±»å‹ | å«ä¹‰ | è¯´æ˜ |
|--------|------|------|------|
| `self.forward_minibatch_id` | int | å½“å‰å‰å‘ä¼ æ’­çš„minibatchç¼–å· | é€’å¢è®¡æ•°å™¨ |
| `self.backward_minibatch_id` | int | å½“å‰åå‘ä¼ æ’­çš„minibatchç¼–å· | é€’å¢è®¡æ•°å™¨ |
| `self.num_warmup_minibatches` | int | warmupé˜¶æ®µçš„minibatchæ•°é‡ | ç”¨äºpipelineå¡«å…… |
| `self.forward_only` | bool | æ˜¯å¦ä»…å‰å‘ä¼ æ’­ | evalæ¨¡å¼ä¸ºTrue |

#### Stageé—´é€šä¿¡
| å˜é‡å | ç±»å‹ | å«ä¹‰ | ç¤ºä¾‹ |
|--------|------|------|------|
| `self.send_ranks` | dict | éœ€è¦å‘é€çš„å¼ é‡åŠç›®æ ‡ranks | `{'out5': [2, 3]}` |
| `self.receive_ranks` | dict | éœ€è¦æ¥æ”¶çš„å¼ é‡åŠæºranks | `{'input0': [0, 1]}` |
| `self.tensor_tags` | dict | å¼ é‡ååˆ°é€šä¿¡tagçš„æ˜ å°„ | `{'input0': 1, 'out5': 2}` |
| `self.ranks_in_previous_stage` | list | å‰ä¸€ä¸ªstageçš„æ‰€æœ‰ranks | `[0, 1]` |
| `self.ranks_in_next_stage` | list | åä¸€ä¸ªstageçš„æ‰€æœ‰ranks | `[4, 5]` |

#### å¼ é‡ç®¡ç†
| å˜é‡å | ç±»å‹ | å«ä¹‰ | è¯´æ˜ |
|--------|------|------|------|
| `self.tensors` | list[dict] | å­˜å‚¨æ¯ä¸ªminibatchçš„æ¿€æ´»å€¼ | `[{'input0': tensor, 'out5': tensor}]` |
| `self.gradients` | dict | å­˜å‚¨æ¢¯åº¦ | `{'out5': gradient_tensor}` |
| `self.tensor_shapes` | dict | å¼ é‡å½¢çŠ¶ä¿¡æ¯ | `{'input0': (batch_size, seq_len, hidden)}` |
| `self.training_tensor_shapes` | dict | è®­ç»ƒæ—¶çš„å¼ é‡å½¢çŠ¶ | ç”¨äºé¢„åˆ†é…buffer |
| `self.eval_tensor_shapes` | dict | è¯„ä¼°æ—¶çš„å¼ é‡å½¢çŠ¶ | å¯èƒ½ä¸è®­ç»ƒä¸åŒ |
| `self.training_tensor_dtypes` | dict | å¼ é‡æ•°æ®ç±»å‹ | `{'input0': torch.float32}` |

#### æ¨¡å‹ç›¸å…³
| å˜é‡å | ç±»å‹ | å«ä¹‰ | è¯´æ˜ |
|--------|------|------|------|
| `self.modules_with_dependencies` | ModulesWithDependencies | å½“å‰stageçš„æ¨¡å—åŠä¾èµ– | åŒ…å«nn.Moduleåˆ—è¡¨ |
| `self.is_criterion` | bool | æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªstage | æœ€åstageåŒ…å«lossè®¡ç®— |
| `self.master_parameters` | list | ä¸»å‚æ•°ï¼ˆä¼˜åŒ–å™¨æ›´æ–°ç”¨ï¼‰ | FP16æ—¶ä¸ºFP32å‰¯æœ¬ |
| `self.model_parameters` | list | æ¨¡å‹å‚æ•°ï¼ˆæ¨ç†ç”¨ï¼‰ | FP16æ¨¡å¼ä¸‹ä½¿ç”¨ |
| `self.enable_recompute` | bool | æ˜¯å¦å¯ç”¨é‡è®¡ç®— | èŠ‚çœæ¿€æ´»å€¼å†…å­˜ |

#### æ€§èƒ½ç»Ÿè®¡
| å˜é‡å | ç±»å‹ | å«ä¹‰ | è¯´æ˜ |
|--------|------|------|------|
| `self.forward_stats` | RuntimeStats | å‰å‘ä¼ æ’­ç»Ÿè®¡ä¿¡æ¯ | æ—¶é—´ã€é€šä¿¡é‡ç­‰ |
| `self.backward_stats` | RuntimeStats | åå‘ä¼ æ’­ç»Ÿè®¡ä¿¡æ¯ | æ—¶é—´ã€é€šä¿¡é‡ç­‰ |
| `self.verbose_freq` | int | è¯¦ç»†æ—¥å¿—æ‰“å°é¢‘ç‡ | æ¯Nä¸ªiterationæ‰“å° |

#### è®­ç»ƒè¶…å‚æ•°
| å˜é‡å | ç±»å‹ | å«ä¹‰ | è¯´æ˜ |
|--------|------|------|------|
| `self.fp16` | bool | æ˜¯å¦ä½¿ç”¨FP16è®­ç»ƒ | èŠ‚çœå†…å­˜å’ŒåŠ é€Ÿ |
| `self.loss_scale` | float | lossç¼©æ”¾å› å­ | FP16è®­ç»ƒé˜²æ­¢ä¸‹æº¢ |
| `self.distributed_backend` | str | åˆ†å¸ƒå¼åç«¯ | 'gloo' æˆ– 'nccl' |

---

## CommunicationHandler ç±»æ ¸å¿ƒå˜é‡ (communication.py)

| å˜é‡å | ç±»å‹ | å«ä¹‰ | è¯´æ˜ |
|--------|------|------|------|
| `self.rank` | int | å…¨å±€rank | ä¸StageRuntimeä¸€è‡´ |
| `self.local_rank` | int | æœ¬åœ°rank | ç”¨äºæŒ‡å®šGPU |
| `self.world_size` | int | å…¨å±€è¿›ç¨‹æ€»æ•° | ç­‰äºæ€»GPUæ•° |
| `self.num_ranks_in_server` | int | å•èŠ‚ç‚¹å†…çš„GPUæ•° | ç”¨äºåˆ¤æ–­æ˜¯å¦GPU-GPUé€šä¿¡ |
| `self.backend` | str | é€šä¿¡åç«¯ | 'gloo' æˆ– 'nccl' |
| `self.ranks_in_server` | list | åŒèŠ‚ç‚¹å†…å…¶ä»–ranks | ç”¨äºå¹¿æ’­é€šä¿¡ |
| `self.connection_list` | list | GPU-GPUé€šä¿¡çš„è¿æ¥ä¿¡æ¯ | `[[tag, rank], ...]` |
| `self.process_groups` | dict | è¿›ç¨‹ç»„å­—å…¸ | ç”¨äºå¹¿æ’­æ“ä½œ |
| `self.tensor_tags` | dict | å¼ é‡é€šä¿¡tagæ˜ å°„ | ä»StageRuntimeä¼ å…¥ |
| `self.target_tensor_names` | set | ç›®æ ‡å¼ é‡åç§°é›†åˆ | å¦‚labels |

### é€šä¿¡é˜Ÿåˆ—ç›¸å…³
| å˜é‡å | ç±»å‹ | å«ä¹‰ | è¯´æ˜ |
|--------|------|------|------|
| `self.forward_receive_queues` | dict | å‰å‘æ¥æ”¶é˜Ÿåˆ— | `{tensor_name: Queue}` |
| `self.backward_receive_queues` | dict | åå‘æ¥æ”¶é˜Ÿåˆ— | `{tensor_name: Queue}` |
| `self.forward_send_queues` | dict | å‰å‘å‘é€é˜Ÿåˆ— | `{tensor_name: Queue}` |
| `self.backward_send_queues` | dict | åå‘å‘é€é˜Ÿåˆ— | `{tensor_name: Queue}` |
| `self.num_forward_threads` | int | å‰å‘é€šä¿¡çº¿ç¨‹æ•° | ç­‰äºå¾…æ¥æ”¶tensoræ•° |
| `self.num_backward_threads` | int | åå‘é€šä¿¡çº¿ç¨‹æ•° | ç­‰äºå¾…æ¥æ”¶tensoræ•° |

### æ¶ˆæ¯ç´¢å¼•ï¼ˆç”¨äºæ•°æ®å¹¶è¡Œï¼‰
| å˜é‡å | ç±»å‹ | å«ä¹‰ | è¯´æ˜ |
|--------|------|------|------|
| `self.messaging_index_receive` | int | æ¥æ”¶æ¶ˆæ¯çš„æºrankç´¢å¼• | è½®è¯¢ä¸åŒæº |
| `self.messaging_index_send` | int | å‘é€æ¶ˆæ¯çš„ç›®æ ‡rankç´¢å¼• | è½®è¯¢ä¸åŒç›®æ ‡ |

---

## OptimizerWithWeightStashing ç±»æ ¸å¿ƒå˜é‡ (optimizer.py)

| å˜é‡å | ç±»å‹ | å«ä¹‰ | è¯´æ˜ |
|--------|------|------|------|
| `self.base_optimizer` | torch.optim.Optimizer | åŸºç¡€ä¼˜åŒ–å™¨ | Adam/SGD/LAMBç­‰ |
| `self.modules` | list | æ¨¡å‹æ¨¡å—åˆ—è¡¨ | ç”¨äºstate_dictæ“ä½œ |
| `self.master_parameters` | list | ä¸»å‚æ•° | ä¼˜åŒ–å™¨ç›´æ¥æ›´æ–°çš„å‚æ•° |
| `self.model_parameters` | list | æ¨¡å‹å‚æ•° | FP16æ¨¡å¼ä¸‹çš„FP16å‰¯æœ¬ |
| `self.loss_scale` | float | lossç¼©æ”¾ | FP16è®­ç»ƒç”¨ |
| `self.num_versions` | int | å­˜å‚¨çš„æƒé‡ç‰ˆæœ¬æ•° | é€šå¸¸ç­‰äºpipelineæ·±åº¦ |
| `self.latest_version` | Version | æœ€æ–°æƒé‡ç‰ˆæœ¬å· | é€’å¢çš„ç‰ˆæœ¬å¯¹è±¡ |
| `self.current_version` | Version | å½“å‰ä½¿ç”¨çš„ç‰ˆæœ¬å· | æ»åäºlatest |
| `self.queue` | deque | æƒé‡ç‰ˆæœ¬é˜Ÿåˆ— | å­˜å‚¨state_dictçš„å¾ªç¯é˜Ÿåˆ— |
| `self.update_interval` | int | æƒé‡æ›´æ–°é—´éš” | macrobatchæ¨¡å¼ä¸‹>1 |
| `self.batch_counter` | int | batchè®¡æ•°å™¨ | ç”¨äºæ§åˆ¶æ›´æ–°é¢‘ç‡ |

---

## Configuration Maps (é…ç½®æ˜ å°„æ•°æ®ç»“æ„)

### module_to_stage_map
**ç±»å‹**: `list[int]`  
**å«ä¹‰**: æ¯ä¸ªæ¨¡å—å¯¹åº”çš„stageç¼–å·  
**ç¤ºä¾‹**:
```python
[0, 0, 0, ..., 0, 1, 1, ..., 1, 2, 2, ..., 2, 3, 3, ..., 3]
# å‰12ä¸ªæ¨¡å—åœ¨stage 0ï¼Œæ¥ä¸‹æ¥13ä¸ªåœ¨stage 1ï¼Œä»¥æ­¤ç±»æ¨
```

### stage_to_rank_map
**ç±»å‹**: `dict[int, list[int]]`  
**å«ä¹‰**: æ¯ä¸ªstageåŒ…å«çš„rankåˆ—è¡¨  
**ç¤ºä¾‹**:
```python
{
    0: [0, 1],      # Stage 0åœ¨rank 0å’Œ1ä¸Šï¼ˆæ•°æ®å¹¶è¡Œï¼‰
    1: [2, 3],      # Stage 1åœ¨rank 2å’Œ3ä¸Š
    2: [4, 5],      # Stage 2åœ¨rank 4å’Œ5ä¸Š
    3: [6, 7]       # Stage 3åœ¨rank 6å’Œ7ä¸Š
}
```

### stage_to_depth_map
**ç±»å‹**: `dict[str, int]`  
**å«ä¹‰**: æ¯ä¸ªstageçš„warmupæ·±åº¦ï¼ˆä»partitionæ–‡ä»¶è®¡ç®—å¾—å‡ºï¼‰  
**ç¤ºä¾‹**:
```python
{
    "0": 0,   # Stage 0ä¸éœ€è¦warmup
    "1": 2,   # Stage 1éœ€è¦2ä¸ªminibatchçš„warmup
    "2": 4,   # Stage 2éœ€è¦4ä¸ªminibatchçš„warmup
    "3": 6    # Stage 3éœ€è¦6ä¸ªminibatchçš„warmup
}
```

---

## Partition é…ç½® (JSONæ–‡ä»¶)

### vpipe.json ç»“æ„
```json
{
    "partition": [12, 13, 13, 11],
    "recompute_ratio": [0.3, 0, 0, 0]
}
```

| å­—æ®µ | ç±»å‹ | å«ä¹‰ | è¯´æ˜ |
|------|------|------|------|
| `partition` | list[int] | æ¯ä¸ªstageçš„å±‚æ•° | æ€»å’Œåº”ç­‰äºæ¨¡å‹æ€»å±‚æ•° |
| `recompute_ratio` | list[float] | æ¯ä¸ªstageçš„é‡è®¡ç®—æ¯”ä¾‹ | 0.0-1.0ï¼Œé€šå¸¸åªæœ‰ç¬¬ä¸€ä¸ªstage>0 |

---

## YAML é…ç½®æ–‡ä»¶å…³é”®å­—æ®µ

| å­—æ®µ | ç±»å‹ | å«ä¹‰ | ç¤ºä¾‹ |
|------|------|------|------|
| `directory` | str | æ¨¡å‹ä»£ç ç›®å½• | 'bert' |
| `module` | str | æ¨¡å—åç§° | 'vgpus=4' |
| `partition` | str | åˆ†åŒºé…ç½®æ–‡ä»¶è·¯å¾„ | 'vgpus=4/vpipe.json' |
| `config_file` | str | æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ | 'vgpus=4/mp_conf.json' |
| `machines` | list[str] | æœºå™¨:GPUåˆ—è¡¨ | ['localhost:0', 'localhost:1'] |
| `sync_mode` | str | åŒæ­¥æ¨¡å¼ | 'asp' / 'bsp' |
| `distributed_backend` | str | åˆ†å¸ƒå¼åç«¯ | 'gloo' / 'nccl' |
| `model_type` | str | æ¨¡å‹ç±»å‹ | 'bert' / 'translation' |
| `data_dir` | str | æ•°æ®é›†ç›®å½• | '/path/to/dataset' |
| `log_directory` | str | æ—¥å¿—è¾“å‡ºç›®å½• | '/path/to/logs' |
| `container` | str | Singularityå®¹å™¨è·¯å¾„ | '/path/to/vpipe.sif' |
| `batch_size` | int | å…¨å±€batchå¤§å° | 16 |
| `learning_rate` | float | å­¦ä¹ ç‡ | 0.03 |
| `learning_rate_policy` | str | å­¦ä¹ ç‡ç­–ç•¥ | 'polynomial' / 'step' |
| `weight_decay` | float | æƒé‡è¡°å‡ | 0.01 |
| `epochs` | int | è®­ç»ƒè½®æ•° | 2 |
| `print_frequency` | int | æ‰“å°é¢‘ç‡ | 100 |
| `verbose_frequency` | int | è¯¦ç»†æ—¥å¿—é¢‘ç‡ | 0 (ä¸æ‰“å°) |
| `recompute` | bool | æ˜¯å¦å¯ç”¨é‡è®¡ç®— | true / false |
| `macrobatch` | bool | æ˜¯å¦å¯ç”¨macrobatch | true / false |
| `synthetic_data` | bool | æ˜¯å¦ä½¿ç”¨åˆæˆæ•°æ® | true / false |

---

## å¸¸ç”¨å‡½æ•°ç­¾å

### runtime.py

```python
def receive_tensors_forward():
    """æ¥æ”¶å‰å‘ä¼ æ’­æ‰€éœ€çš„è¾“å…¥å¼ é‡"""
    
def send_tensors_forward():
    """å‘é€å‰å‘ä¼ æ’­çš„è¾“å‡ºå¼ é‡åˆ°ä¸‹æ¸¸stage"""
    
def run_forward(recompute_step=False):
    """æ‰§è¡Œå‰å‘ä¼ æ’­
    Args:
        recompute_step: æ˜¯å¦ä¸ºé‡è®¡ç®—æ­¥éª¤ï¼ˆåå‘æ—¶ä½¿ç”¨ï¼‰
    """
    
def receive_tensors_backward():
    """æ¥æ”¶åå‘ä¼ æ’­çš„æ¢¯åº¦å¼ é‡"""
    
def send_tensors_backward():
    """å‘é€åå‘ä¼ æ’­çš„æ¢¯åº¦åˆ°ä¸Šæ¸¸stage"""
    
def run_backward():
    """æ‰§è¡Œåå‘ä¼ æ’­"""
    
def run_training_iteration(num_iterations, start_iter=0):
    """è¿è¡Œè®­ç»ƒè¿­ä»£
    Args:
        num_iterations: æ€»è¿­ä»£æ¬¡æ•°
        start_iter: èµ·å§‹è¿­ä»£ç¼–å·
    Returns:
        (iteration_time, loss): è¿­ä»£æ—¶é—´å’Œlosså€¼
    """
```

### communication.py

```python
def send(tensor_name, tensor, forward_minibatch_id, 
         backward_minibatch_id, backward=False):
    """å‘é€å¼ é‡
    Args:
        tensor_name: å¼ é‡åç§°
        tensor: è¦å‘é€çš„å¼ é‡
        forward_minibatch_id: å‰å‘minibatch ID
        backward_minibatch_id: åå‘minibatch ID
        backward: æ˜¯å¦ä¸ºåå‘ä¼ æ’­
    """

def recv(tensor_name, forward_minibatch_id, 
         backward_minibatch_id, backward=False):
    """æ¥æ”¶å¼ é‡
    Args:
        tensor_name: å¼ é‡åç§°
        forward_minibatch_id: å‰å‘minibatch ID
        backward_minibatch_id: åå‘minibatch ID
        backward: æ˜¯å¦ä¸ºåå‘ä¼ æ’­
    Returns:
        received_tensor: æ¥æ”¶åˆ°çš„å¼ é‡
    """

def wait():
    """ç­‰å¾…æ‰€æœ‰é€šä¿¡å®Œæˆ"""

def start_helper_threads(num_iterations, forward_only):
    """å¯åŠ¨åå°é€šä¿¡çº¿ç¨‹
    Args:
        num_iterations: æ€»è¿­ä»£æ¬¡æ•°
        forward_only: æ˜¯å¦ä»…å‰å‘ä¼ æ’­
    """
```

### optimizer.py

```python
def load_old_params():
    """åŠ è½½æ—§ç‰ˆæœ¬çš„æƒé‡ï¼ˆç”¨äºåº”ç”¨æ¢¯åº¦ï¼‰"""

def load_new_params():
    """åŠ è½½æ–°ç‰ˆæœ¬çš„æƒé‡ï¼ˆç”¨äºå‰å‘ä¼ æ’­ï¼‰"""

def step(update_master_weights=True):
    """æ‰§è¡Œä¼˜åŒ–å™¨step
    Args:
        update_master_weights: æ˜¯å¦æ›´æ–°ä¸»æƒé‡
    """
```

---

## è°ƒè¯•å¸¸ç”¨æ‰“å°è¯­å¥

### è¿½è¸ªæ‰§è¡Œæµç¨‹
```python
print(f"[Rank {self.rank}|Stage {self.stage}] "
      f"Forward minibatch {self.forward_minibatch_id}")
```

### æŸ¥çœ‹é€šä¿¡çŠ¶æ€
```python
print(f"[Rank {self.rank}] Sending {tensor_name} to {self.send_ranks[tensor_name]}")
print(f"[Rank {self.rank}] Receiving {tensor_name} from {self.receive_ranks[tensor_name]}")
```

### æŸ¥çœ‹å¼ é‡å½¢çŠ¶
```python
print(f"Tensor {name}: shape={tensor.shape}, device={tensor.device}")
```

### æŸ¥çœ‹ç‰ˆæœ¬ä¿¡æ¯
```python
print(f"[Optimizer] Current version: {self.current_version}, "
      f"Latest version: {self.latest_version}")
```

### æŸ¥çœ‹æ—¶é—´ç»Ÿè®¡
```python
print(f"Forward time: {self.forward_stats.stats['forward_time']:.4f}s")
print(f"Backward time: {self.backward_stats.stats['backward_time']:.4f}s")
print(f"Communication size: {self.forward_stats.stats['receive_tensors_size']/1024/1024:.2f} MB")
```

---

## å…¸å‹è°ƒè¯•åœºæ™¯

### åœºæ™¯1: è¿½è¸ªä¸€ä¸ªtensorçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ
```python
# åœ¨ runtime.py ä¸­æ·»åŠ 
def send_tensors_forward(self):
    for output_name in self.send_ranks:
        tensor = self.tensors[-1][output_name]
        print(f"[{self.rank}] Sending {output_name}: "
              f"shape={tensor.shape}, norm={tensor.norm():.4f}")
        # ... åŸæœ‰ä»£ç 
```

### åœºæ™¯2: éªŒè¯æƒé‡ç‰ˆæœ¬åˆ‡æ¢
```python
# åœ¨ optimizer.py ä¸­æ·»åŠ 
def load_old_params(self):
    old_version = self.current_version
    # ... åŸæœ‰ä»£ç 
    print(f"[Optimizer] Loaded old params: version {old_version}")
```

### åœºæ™¯3: æ£€æŸ¥stageåˆ†é…æ˜¯å¦æ­£ç¡®
```python
# åœ¨ runtime.py çš„ __init__ ä¸­æ·»åŠ 
if self.rank == 0:
    for stage_id, ranks in stage_to_rank_map.items():
        modules = stage_to_module_map[stage_id]
        print(f"Stage {stage_id}: ranks={ranks}, modules={modules}")
```

---

## å¿«é€Ÿè¯Šæ–­é—®é¢˜

| ç°è±¡ | å¯èƒ½åŸå›  | æ£€æŸ¥å˜é‡ |
|------|----------|----------|
| è®­ç»ƒå¡ä½ | é€šä¿¡æ­»é” | `send_ranks`, `receive_ranks` |
| OOM (å†…å­˜æº¢å‡º) | batchå¤ªå¤§æˆ–æ¿€æ´»å€¼è¿‡å¤š | `batch_size`, `recompute_ratio` |
| Lossä¸æ”¶æ•› | å­¦ä¹ ç‡ä¸å½“æˆ–æ¢¯åº¦ç‰ˆæœ¬é”™è¯¯ | `learning_rate`, `num_versions` |
| ååé‡ä½ | partitionä¸å¹³è¡¡ | `partition`, å„stageçš„å±‚æ•° |
| é€šä¿¡å¼€é”€å¤§ | stageåˆ‡åˆ†è¿‡ç»† | `num_stages`, `partition` |
| ç‰ˆæœ¬é”™è¯¯ | weight stashingé—®é¢˜ | `latest_version`, `current_version` |

---

## æ€»ç»“

è¿™ä¸ªé€ŸæŸ¥è¡¨æ¶µç›–äº†VPipeä¸­æœ€é‡è¦çš„å˜é‡å’Œæ•°æ®ç»“æ„ã€‚å»ºè®®ï¼š

1. **åˆå­¦è€…**: é‡ç‚¹å…³æ³¨ `rank`, `stage`, `send_ranks`, `receive_ranks`
2. **è¿›é˜¶å­¦ä¹ **: æ·±å…¥ç†è§£ `num_versions`, `queue`, `tensor_tags`
3. **æ€§èƒ½è°ƒä¼˜**: å…³æ³¨ `partition`, `recompute_ratio`, `batch_size`

åœ¨é˜…è¯»ä»£ç æ—¶ï¼Œå¯ä»¥å°†æœ¬æ–‡æ¡£ä½œä¸ºå‚è€ƒï¼Œå¿«é€Ÿç†è§£å˜é‡å«ä¹‰ã€‚

