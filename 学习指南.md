# VPipe é¡¹ç›®å­¦ä¹ æŒ‡å—

## ğŸ“š é¡¹ç›®æ¦‚è¿°

VPipe æ˜¯ä¸€ä¸ªåŸºäº**æµæ°´çº¿å¹¶è¡Œ**çš„æ·±åº¦å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œä¸“æ³¨äºè§£å†³å¤§æ¨¡å‹è®­ç»ƒä¸­çš„å†…å­˜å’Œé€šä¿¡ç“¶é¢ˆé—®é¢˜ã€‚ç›¸æ¯” GPipe å’Œ PipeDreamï¼ŒVPipe æä¾›äº†æ›´ä¼˜åŒ–çš„å¹¶è¡Œç­–ç•¥ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism)**: å°†æ¨¡å‹æŒ‰å±‚åˆ‡åˆ†åˆ°ä¸åŒGPU
- **å¼‚æ­¥è°ƒåº¦ (Asynchronous Scheduling)**: é‡‡ç”¨ ASP (Asynchronous Stale Parameter) æ¨¡å¼
- **æƒé‡å­˜å‚¨ (Weight Stashing)**: è§£å†³å¼‚æ­¥è®­ç»ƒä¸­çš„æ¢¯åº¦ä¸€è‡´æ€§é—®é¢˜
- **æ¿€æ´»é‡è®¡ç®— (Activation Recomputation)**: èŠ‚çœå†…å­˜å¼€é”€
- **æ•°æ®å¹¶è¡Œ**: åŒä¸€stageå†…æ”¯æŒå¤šGPUæ•°æ®å¹¶è¡Œ

---

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
vpipe/
â”œâ”€â”€ runtime/              # æ ¸å¿ƒè¿è¡Œæ—¶ç³»ç»Ÿ
â”‚   â”œâ”€â”€ driver.py        # ä¸»å¯åŠ¨å™¨ï¼šè§£æé…ç½®ã€å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
â”‚   â”œâ”€â”€ runtime.py       # è¿è¡Œæ—¶å¼•æ“ï¼šç®¡ç†å‰å‘/åå‘ä¼ æ’­
â”‚   â”œâ”€â”€ communication.py # é€šä¿¡ç®¡ç†ï¼šå¤„ç†è·¨GPU/è·¨èŠ‚ç‚¹æ•°æ®ä¼ è¾“
â”‚   â”œâ”€â”€ optimizer.py     # ä¼˜åŒ–å™¨åŒ…è£…ï¼šå®ç°weight stashing
â”‚   â”œâ”€â”€ checkpoint.py    # æ£€æŸ¥ç‚¹ç®¡ç†
â”‚   â”œâ”€â”€ bert/           # BERTæ¨¡å‹å®ç°
â”‚   â”‚   â”œâ”€â”€ vpipe.py    # æ¨¡å‹åˆ‡åˆ†é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ main_with_runtime.py  # è®­ç»ƒä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ modeling.py # BERTæ¨¡å‹å®šä¹‰
â”‚   â”‚   â””â”€â”€ vgpus=4/    # 4-GPUé…ç½®
â”‚   â”‚       â”œâ”€â”€ vpipe.json      # åˆ†åŒºé…ç½®
â”‚   â”‚       â””â”€â”€ mp_conf.json    # æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ configs/        # å®éªŒé…ç½®æ–‡ä»¶
â”œâ”€â”€ cpm/                 # GPT-2ä¸­æ–‡æ¨¡å‹ï¼ˆæ­£åœ¨å¼€å‘ï¼‰
â””â”€â”€ pytorch/            # PyTorchç›¸å…³å·¥å…·
```

---

## ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µä¸å˜é‡å«ä¹‰

### 1. Stage (é˜¶æ®µ)
**å«ä¹‰**: æ¨¡å‹çš„ä¸€ä¸ªåˆ†åŒºï¼ŒåŒ…å«è¿ç»­çš„è‹¥å¹²å±‚ï¼Œéƒ¨ç½²åœ¨ä¸€ä¸ªæˆ–å¤šä¸ªGPUä¸Š

**å…³é”®å˜é‡**:
```python
self.stage              # å½“å‰rankæ‰€å±çš„stage ID (0, 1, 2, ...)
self.num_stages         # æ€»stageæ•°é‡
self.rank_in_stage      # å½“å‰rankåœ¨stageå†…çš„æ’å
self.num_ranks_in_stage # åŒä¸€stageå†…çš„GPUæ•°é‡ï¼ˆæ•°æ®å¹¶è¡Œåº¦ï¼‰
```

### 2. Rank (ç§©)
**å«ä¹‰**: åˆ†å¸ƒå¼è®­ç»ƒä¸­æ¯ä¸ªè¿›ç¨‹çš„å”¯ä¸€æ ‡è¯†ç¬¦

**å…³é”®å˜é‡**:
```python
self.rank          # å…¨å±€rank ID
self.local_rank    # èŠ‚ç‚¹å†…çš„local rankï¼ˆç”¨äºæŒ‡å®šGPUè®¾å¤‡ï¼‰
self.num_ranks     # æ€»GPUæ•°é‡
```

### 3. Minibatch (å¾®æ‰¹æ¬¡)
**å«ä¹‰**: åœ¨æµæ°´çº¿å¹¶è¡Œä¸­ï¼Œä¸€ä¸ªbatchä¼šè¢«åˆ‡åˆ†æˆå¤šä¸ªminibatchä¾æ¬¡å¤„ç†

**å…³é”®å˜é‡**:
```python
self.forward_minibatch_id   # å½“å‰å‰å‘ä¼ æ’­çš„minibatch ID
self.backward_minibatch_id  # å½“å‰åå‘ä¼ æ’­çš„minibatch ID
self.num_warmup_minibatches # warmupé˜¶æ®µçš„minibatchæ•°é‡
```

### 4. Tensor Communication (å¼ é‡é€šä¿¡)
**å«ä¹‰**: stageé—´ä¼ é€’æ¿€æ´»å€¼å’Œæ¢¯åº¦

**å…³é”®å˜é‡**:
```python
self.send_ranks         # éœ€è¦å‘é€å¼ é‡çš„ç›®æ ‡rankså­—å…¸
self.receive_ranks      # éœ€è¦æ¥æ”¶å¼ é‡çš„æºrankså­—å…¸
self.tensor_tags        # æ¯ä¸ªå¼ é‡çš„å”¯ä¸€é€šä¿¡tag
self.tensor_shapes      # å¼ é‡å½¢çŠ¶ä¿¡æ¯ï¼ˆç”¨äºé¢„åˆ†é…bufferï¼‰
```

### 5. Weight Stashing (æƒé‡å­˜å‚¨)
**å«ä¹‰**: ä¿å­˜å¤šä¸ªç‰ˆæœ¬çš„æ¨¡å‹æƒé‡ï¼Œç¡®ä¿å¼‚æ­¥è®­ç»ƒä¸­æ¢¯åº¦å¯¹åº”æ­£ç¡®çš„æƒé‡ç‰ˆæœ¬

**å…³é”®å˜é‡**:
```python
self.num_versions       # å­˜å‚¨çš„æƒé‡ç‰ˆæœ¬æ•°
self.latest_version     # æœ€æ–°çš„æƒé‡ç‰ˆæœ¬
self.current_version    # å½“å‰ä½¿ç”¨çš„æƒé‡ç‰ˆæœ¬
self.queue             # æƒé‡ç‰ˆæœ¬é˜Ÿåˆ—ï¼ˆdequeå®ç°ï¼‰
```

### 6. Recomputation (é‡è®¡ç®—)
**å«ä¹‰**: ä¸ä¿å­˜å‰å‘ä¼ æ’­çš„ä¸­é—´æ¿€æ´»å€¼ï¼Œåå‘æ—¶é‡æ–°è®¡ç®—ä»¥èŠ‚çœå†…å­˜

**å…³é”®å˜é‡**:
```python
self.enable_recompute   # æ˜¯å¦å¯ç”¨é‡è®¡ç®—
recompute_ratio        # é‡è®¡ç®—çš„å±‚æ•°æ¯”ä¾‹ï¼ˆåœ¨partitioné…ç½®ä¸­ï¼‰
```

### 7. Configuration Maps (é…ç½®æ˜ å°„)
**å…³é”®æ•°æ®ç»“æ„**:
```python
module_to_stage_map    # æ¨¡å—ID -> Stage ID çš„æ˜ å°„
stage_to_rank_map      # Stage ID -> Rankåˆ—è¡¨ çš„æ˜ å°„
stage_to_depth_map     # Stage ID -> warmupæ·±åº¦ çš„æ˜ å°„
```

---

## ğŸ“– å…³é”®é…ç½®æ–‡ä»¶è¯¦è§£

### 1. YAMLé…ç½®æ–‡ä»¶ (ä¾‹: bert_4vpipe.yml)
```yaml
directory: 'bert'                    # æ¨¡å‹ä»£ç ç›®å½•
module: 'vgpus=4'                    # æ¨¡å—åç§°ï¼ˆå¯¹åº”vgpus=4æ–‡ä»¶å¤¹ï¼‰
partition: 'vgpus=4/vpipe.json'      # åˆ†åŒºé…ç½®æ–‡ä»¶
config_file: 'vgpus=4/mp_conf.json'  # æ¨¡å‹é…ç½®æ–‡ä»¶

machines: ['localhost:0', 'localhost:1', 'localhost:2', 'localhost:3']
# æ ¼å¼: IPåœ°å€:GPU_ID

sync_mode: 'asp'                     # åŒæ­¥æ¨¡å¼: asp(å¼‚æ­¥) / bsp(åŒæ­¥)
distributed_backend: 'gloo'          # é€šä¿¡åç«¯: gloo / nccl

batch_size: 16                       # å…¨å±€batchå¤§å°
learning_rate: 0.03                  # å­¦ä¹ ç‡
epochs: 2                            # è®­ç»ƒè½®æ•°
```

### 2. åˆ†åŒºé…ç½®æ–‡ä»¶ (vpipe.json)
```json
{
    "partition": [12, 13, 13, 11],      // æ¯ä¸ªstageåŒ…å«çš„å±‚æ•°
    "recompute_ratio": [0.3, 0, 0, 0]   // æ¯ä¸ªstageçš„é‡è®¡ç®—æ¯”ä¾‹
}
```
**è§£é‡Š**:
- `partition`: 49å±‚BERTè¢«åˆ†æˆ4ä¸ªstageï¼Œåˆ†åˆ«åŒ…å«12/13/13/11å±‚
- `recompute_ratio`: ç¬¬ä¸€ä¸ªstageé‡è®¡ç®—30%çš„å±‚ï¼ˆæœ€åçš„å±‚ï¼‰ï¼Œå…¶ä»–stageä¸é‡è®¡ç®—

---

## ğŸ¯ æ¨èå­¦ä¹ è·¯å¾„

### é˜¶æ®µä¸€: ç†è§£æ•´ä½“æµç¨‹ (2-3å¤©)

#### 1. ä»å¯åŠ¨åˆ°è¿è¡Œçš„å®Œæ•´é“¾è·¯
æŒ‰ç…§ä»¥ä¸‹é¡ºåºé˜…è¯»ä»£ç ï¼š

**Step 1: driver.py (å¯åŠ¨å™¨)**
- å…³æ³¨ `main` å‡½æ•°çš„æ‰§è¡Œæµç¨‹
- ç†è§£å¦‚ä½•è§£æYAMLé…ç½®
- è§‚å¯Ÿå¦‚ä½•æ„å»ºåˆ†å¸ƒå¼å¯åŠ¨å‘½ä»¤

**Step 2: main_with_runtime.py (è®­ç»ƒä¸»ç¨‹åº)**
- ä» `main()` å‡½æ•°å¼€å§‹
- é‡ç‚¹çœ‹ `stage()` å‡½æ•°ï¼šæ¨¡å‹çš„å®šä¹‰å’Œåˆ†åŒº
- ç†è§£ `train()` å‡½æ•°ï¼šè®­ç»ƒå¾ªç¯é€»è¾‘

**Step 3: runtime.py (è¿è¡Œæ—¶æ ¸å¿ƒ)**
- å…ˆçœ‹ `StageRuntime.__init__()`: åˆå§‹åŒ–é€»è¾‘
- å†çœ‹ `train()` æ–¹æ³•ï¼šå¯åŠ¨è®­ç»ƒ
- æœ€åçœ‹ `run_training_iteration()`: ä¸€æ¬¡è¿­ä»£çš„å®Œæ•´æµç¨‹

**å­¦ä¹ å»ºè®®**:
```bash
# åœ¨ä»£ç ä¸­æ·»åŠ æ‰“å°è¯­å¥ï¼Œè§‚å¯Ÿæ‰§è¡Œæµç¨‹
python driver.py --config_file configs/bert_4vpipe.yml
```

### é˜¶æ®µäºŒ: æ·±å…¥æ ¸å¿ƒæœºåˆ¶ (3-4å¤©)

#### 2. æµæ°´çº¿è°ƒåº¦æœºåˆ¶
**é‡ç‚¹æ–‡ä»¶**: `runtime.py` çš„ `run_training_iteration()`

å…³é”®æ–¹æ³•ï¼š
```python
receive_tensors_forward()    # æ¥æ”¶ä¸Šæ¸¸æ¿€æ´»å€¼
run_forward()                # æ‰§è¡Œå‰å‘ä¼ æ’­
send_tensors_forward()       # å‘é€æ¿€æ´»å€¼åˆ°ä¸‹æ¸¸

receive_tensors_backward()   # æ¥æ”¶ä¸‹æ¸¸æ¢¯åº¦
run_backward()               # æ‰§è¡Œåå‘ä¼ æ’­
send_tensors_backward()      # å‘é€æ¢¯åº¦åˆ°ä¸Šæ¸¸
```

**å­¦ä¹ ä»»åŠ¡**:
- ç»˜åˆ¶4ä¸ªstageçš„æ—¶é—´çº¿å›¾ï¼Œæ ‡æ³¨æ¯ä¸ªstageåœ¨æ¯ä¸ªæ—¶åˆ»çš„æ“ä½œ
- ç†è§£warmupé˜¶æ®µå’Œç¨³å®šé˜¶æ®µçš„åŒºåˆ«

#### 3. é€šä¿¡æœºåˆ¶
**é‡ç‚¹æ–‡ä»¶**: `communication.py`

æ ¸å¿ƒæ¦‚å¿µï¼š
- **ç‚¹å¯¹ç‚¹é€šä¿¡**: `send()` / `recv()` ç”¨äºè·¨èŠ‚ç‚¹stageé—´é€šä¿¡
- **å¹¿æ’­é€šä¿¡**: ç”¨äºåŒä¸€stageå†…çš„æ•°æ®å¹¶è¡Œ
- **å¼‚æ­¥çº¿ç¨‹**: `start_helper_threads()` å¯åŠ¨åå°é€šä¿¡çº¿ç¨‹

**å­¦ä¹ ä»»åŠ¡**:
```python
# ç†è§£ CommunicationHandler çš„è¿™äº›æ–¹æ³•
register_tensor()           # æ³¨å†Œéœ€è¦é€šä¿¡çš„å¼ é‡
wait()                      # ç­‰å¾…é€šä¿¡å®Œæˆ
send() / recv()             # å‘é€/æ¥æ”¶å¼ é‡
```

#### 4. æƒé‡å­˜å‚¨æœºåˆ¶
**é‡ç‚¹æ–‡ä»¶**: `optimizer.py` çš„ `OptimizerWithWeightStashing`

**å­¦ä¹ ä»»åŠ¡**:
- ç†è§£ä¸ºä»€ä¹ˆéœ€è¦weight stashingï¼ˆå¼‚æ­¥è®­ç»ƒä¸­æ¢¯åº¦å¯¹åº”çš„æƒé‡ç‰ˆæœ¬é—®é¢˜ï¼‰
- è¿½è¸ª `step()` æ–¹æ³•ï¼šä½•æ—¶æ›´æ–°æƒé‡ç‰ˆæœ¬
- ç†è§£ `load_old_params()` å’Œ `load_new_params()`: ç‰ˆæœ¬åˆ‡æ¢é€»è¾‘

### é˜¶æ®µä¸‰: æ¨¡å‹å®ç°ç»†èŠ‚ (2-3å¤©)

#### 5. BERTæ¨¡å‹åˆ‡åˆ†
**é‡ç‚¹æ–‡ä»¶**: `bert/vpipe.py`

æ ¸å¿ƒç±»ï¼š
```python
class Bert:
    generate_layer_blocks()    # å°†BERTä»£ç è§£æä¸ºblock
    generate_stage()           # æ ¹æ®partitionç”Ÿæˆstageä»£ç 

class Stage(torch.nn.Module):
    forward()                  # stageçš„å‰å‘ä¼ æ’­
    cp_forward()              # å¸¦checkpointçš„å‰å‘ä¼ æ’­ï¼ˆç”¨äºé‡è®¡ç®—ï¼‰
```

**å­¦ä¹ ä»»åŠ¡**:
- ç†è§£BERTæ˜¯å¦‚ä½•è¢«åˆ‡åˆ†çš„
- æŸ¥çœ‹ç”Ÿæˆçš„stageä»£ç ï¼ˆé€šè¿‡æ‰“å° `self.no_cp` å’Œ `self.cp`ï¼‰
- ç†è§£é‡è®¡ç®—å¦‚ä½•èŠ‚çœå†…å­˜

#### 6. æ•°æ®åŠ è½½
**é‡ç‚¹**: `main_with_runtime.py` ä¸­çš„æ•°æ®åŠ è½½é€»è¾‘

```python
class BertPretrainingDataset    # BERTé¢„è®­ç»ƒæ•°æ®é›†
stage_to_dataloader()           # ä¸ºæ¯ä¸ªstageåˆ›å»ºdataloader
```

**å…³é”®ç‚¹**:
- åªæœ‰ç¬¬ä¸€ä¸ªstageéœ€è¦dataloader
- å…¶ä»–stageé€šè¿‡é€šä¿¡æ¥æ”¶æ¿€æ´»å€¼

### é˜¶æ®µå››: å®éªŒä¸è°ƒä¼˜ (3-5å¤©)

#### 7. è¿è¡Œå®éªŒ
```bash
# 1. è¿è¡ŒVPipe (4 GPU)
cd runtime
python driver.py --config_file configs/bert_4vpipe.yml

# 2. å¯¹æ¯”GPipe
python driver.py --config_file configs/bert_4gpipe.yml

# 3. å¯¹æ¯”PipeDream
python driver.py --config_file configs/bert_4pipedream.yml
```

#### 8. æ€§èƒ½åˆ†æ
**æŸ¥çœ‹æ—¥å¿—**:
```bash
# è¾“å‡ºåœ¨ log_directory æŒ‡å®šçš„ç›®å½•
cat output/<timestamp>/output.log.0
```

**å…³é”®æŒ‡æ ‡**:
- Throughput (samples/sec): ååé‡
- Time per iteration: æ¯æ¬¡è¿­ä»£æ—¶é—´
- Communication time: é€šä¿¡æ—¶é—´å æ¯”

#### 9. å‚æ•°è°ƒä¼˜å®éªŒ
å°è¯•ä¿®æ”¹ä»¥ä¸‹å‚æ•°è§‚å¯Ÿæ•ˆæœï¼š

**batch_size**:
```yaml
batch_size: 8/16/32  # è§‚å¯Ÿååé‡å˜åŒ–
```

**partition**:
```json
// å°è¯•ä¸åŒçš„å±‚åˆ†é…æ–¹æ¡ˆ
{"partition": [10, 15, 15, 9]}  // ä¸å‡åŒ€åˆ†é…
{"partition": [12, 13, 13, 11]} // åŸå§‹é…ç½®
```

**recompute_ratio**:
```json
// å°è¯•ä¸åŒçš„é‡è®¡ç®—æ¯”ä¾‹
{"recompute_ratio": [0.0, 0, 0, 0]}  // ä¸é‡è®¡ç®—
{"recompute_ratio": [0.5, 0, 0, 0]}  // 50%é‡è®¡ç®—
```

---

## ğŸ”¬ æ·±å…¥ç ”ç©¶å»ºè®®

### 1. ä»£ç æ’æ¡©è°ƒè¯•
åœ¨å…³é”®ä½ç½®æ·»åŠ æ—¥å¿—ï¼š
```python
# åœ¨ runtime.py ä¸­
def receive_tensors_forward(self):
    print(f"[Rank {self.rank}] Receiving forward tensors, minibatch_id={self.forward_minibatch_id}")
    # ... åŸæœ‰ä»£ç 
```

### 2. å¯è§†åŒ–æ—¶é—´çº¿
ä¿®æ”¹ `runtime_utilities.py` è®°å½•æ¯ä¸ªæ“ä½œçš„æ—¶é—´æˆ³ï¼š
```python
self.forward_stats.stats['start_time'] = time.time()
self.forward_stats.stats['end_time'] = time.time()
```

### 3. å¯¹æ¯”å®éªŒ
- **VPipe vs GPipe**: è§‚å¯Ÿå¼‚æ­¥è°ƒåº¦çš„ä¼˜åŠ¿
- **VPipe vs PipeDream**: è§‚å¯Ÿweight stashingçš„å¿…è¦æ€§
- **å¯ç”¨/ç¦ç”¨é‡è®¡ç®—**: è§‚å¯Ÿå†…å­˜-æ—¶é—´çš„æƒè¡¡

### 4. æ‰©å±•é˜…è¯»
è®ºæ–‡æ¨èï¼š
- GPipe (NIPS 2019): åŒæ­¥æµæ°´çº¿å¹¶è¡Œ
- PipeDream (SOSP 2019): å¼‚æ­¥æµæ°´çº¿å¹¶è¡Œ
- VPipe è®ºæ–‡ï¼ˆæœç´¢ç›¸å…³æ–‡çŒ®ï¼‰

---

## ğŸ’¡ å¸¸è§é—®é¢˜è§£ç­”

### Q1: Stageã€Rankã€GPUçš„å…³ç³»ï¼Ÿ
**A**: 
- GPU: ç‰©ç†è®¾å¤‡
- Rank: è¿›ç¨‹IDï¼Œæ¯ä¸ªè¿›ç¨‹ç»‘å®šä¸€ä¸ªGPU
- Stage: æ¨¡å‹åˆ†åŒºï¼Œå¯ä»¥åŒ…å«å¤šä¸ªrankï¼ˆæ•°æ®å¹¶è¡Œï¼‰

ä¾‹å¦‚ï¼š8 GPUï¼Œ4 stagesï¼Œæ¯ä¸ªstage 2ä¸ªrank
```
Stage 0: Rank 0, 1  (GPU 0, 1)
Stage 1: Rank 2, 3  (GPU 2, 3)
Stage 2: Rank 4, 5  (GPU 4, 5)
Stage 3: Rank 6, 7  (GPU 6, 7)
```

### Q2: ä¸ºä»€ä¹ˆéœ€è¦ tensor_tagsï¼Ÿ
**A**: åœ¨åˆ†å¸ƒå¼é€šä¿¡ä¸­ï¼Œéœ€è¦ç”¨tagåŒºåˆ†ä¸åŒçš„å¼ é‡ã€‚ä¾‹å¦‚ï¼š
- tag=1: input0
- tag=2: input1
- tag=3: out5
ç¡®ä¿å‘é€æ–¹å’Œæ¥æ”¶æ–¹é€šä¿¡çš„æ˜¯åŒä¸€ä¸ªå¼ é‡ã€‚

### Q3: num_warmup_minibatches çš„ä½œç”¨ï¼Ÿ
**A**: æµæ°´çº¿å¹¶è¡Œéœ€è¦warmupé˜¶æ®µå¡«æ»¡pipelineï¼š
```
Time  Stage0  Stage1  Stage2  Stage3
  0     F0      -       -       -     
  1     F1     F0       -       -     (warmup)
  2     F2     F1      F0       -     (warmup)
  3     F3     F2      F1      F0     (ç¨³å®šçŠ¶æ€)
```
Stage 3çš„warmupæ·±åº¦ä¸º3ã€‚

### Q4: å¦‚ä½•ç†è§£ recompute_ratio=0.3ï¼Ÿ
**A**: 
- Stageæœ‰10ä¸ªblockï¼Œrecompute_ratio=0.3
- å‰7ä¸ªblockæ­£å¸¸ä¿å­˜æ¿€æ´»å€¼
- å3ä¸ªblockçš„æ¿€æ´»å€¼ä¸ä¿å­˜ï¼Œåå‘æ—¶é‡æ–°è®¡ç®—
- èŠ‚çœ30%çš„æ¿€æ´»å€¼å†…å­˜

### Q5: ASP vs BSP æ¨¡å¼åŒºåˆ«ï¼Ÿ
**A**:
- **BSP** (Bulk Synchronous Parallel): æ‰€æœ‰minibatchçš„æ¢¯åº¦åº”ç”¨åæ‰å¼€å§‹ä¸‹ä¸€è½®
- **ASP** (Asynchronous Stale Parameter): æ¯ä¸ªminibatchç‹¬ç«‹åº”ç”¨æ¢¯åº¦ï¼Œå…è®¸ä½¿ç”¨"é™ˆæ—§"çš„æƒé‡
- ASPé€Ÿåº¦æ›´å¿«ä½†éœ€è¦weight stashingä¿è¯æ­£ç¡®æ€§

