# VPipe 项目学习指南

## 📚 项目概述

VPipe 是一个基于**流水线并行**的深度学习训练框架，专注于解决大模型训练中的内存和通信瓶颈问题。相比 GPipe 和 PipeDream，VPipe 提供了更优化的并行策略。

### 核心特性
- **流水线并行 (Pipeline Parallelism)**: 将模型按层切分到不同GPU
- **异步调度 (Asynchronous Scheduling)**: 采用 ASP (Asynchronous Stale Parameter) 模式
- **权重存储 (Weight Stashing)**: 解决异步训练中的梯度一致性问题
- **激活重计算 (Activation Recomputation)**: 节省内存开销
- **数据并行**: 同一stage内支持多GPU数据并行

---

## 🏗️ 项目架构

```
vpipe/
├── runtime/              # 核心运行时系统
│   ├── driver.py        # 主启动器：解析配置、启动分布式训练
│   ├── runtime.py       # 运行时引擎：管理前向/后向传播
│   ├── communication.py # 通信管理：处理跨GPU/跨节点数据传输
│   ├── optimizer.py     # 优化器包装：实现weight stashing
│   ├── checkpoint.py    # 检查点管理
│   ├── bert/           # BERT模型实现
│   │   ├── vpipe.py    # 模型切分逻辑
│   │   ├── main_with_runtime.py  # 训练主程序
│   │   ├── modeling.py # BERT模型定义
│   │   └── vgpus=4/    # 4-GPU配置
│   │       ├── vpipe.json      # 分区配置
│   │       └── mp_conf.json    # 模型配置
│   └── configs/        # 实验配置文件
├── cpm/                 # GPT-2中文模型（正在开发）
└── pytorch/            # PyTorch相关工具
```

---

## 🔑 核心概念与变量含义

### 1. Stage (阶段)
**含义**: 模型的一个分区，包含连续的若干层，部署在一个或多个GPU上

**关键变量**:
```python
self.stage              # 当前rank所属的stage ID (0, 1, 2, ...)
self.num_stages         # 总stage数量
self.rank_in_stage      # 当前rank在stage内的排名
self.num_ranks_in_stage # 同一stage内的GPU数量（数据并行度）
```

### 2. Rank (秩)
**含义**: 分布式训练中每个进程的唯一标识符

**关键变量**:
```python
self.rank          # 全局rank ID
self.local_rank    # 节点内的local rank（用于指定GPU设备）
self.num_ranks     # 总GPU数量
```

### 3. Minibatch (微批次)
**含义**: 在流水线并行中，一个batch会被切分成多个minibatch依次处理

**关键变量**:
```python
self.forward_minibatch_id   # 当前前向传播的minibatch ID
self.backward_minibatch_id  # 当前反向传播的minibatch ID
self.num_warmup_minibatches # warmup阶段的minibatch数量
```

### 4. Tensor Communication (张量通信)
**含义**: stage间传递激活值和梯度

**关键变量**:
```python
self.send_ranks         # 需要发送张量的目标ranks字典
self.receive_ranks      # 需要接收张量的源ranks字典
self.tensor_tags        # 每个张量的唯一通信tag
self.tensor_shapes      # 张量形状信息（用于预分配buffer）
```

### 5. Weight Stashing (权重存储)
**含义**: 保存多个版本的模型权重，确保异步训练中梯度对应正确的权重版本

**关键变量**:
```python
self.num_versions       # 存储的权重版本数
self.latest_version     # 最新的权重版本
self.current_version    # 当前使用的权重版本
self.queue             # 权重版本队列（deque实现）
```

### 6. Recomputation (重计算)
**含义**: 不保存前向传播的中间激活值，反向时重新计算以节省内存

**关键变量**:
```python
self.enable_recompute   # 是否启用重计算
recompute_ratio        # 重计算的层数比例（在partition配置中）
```

### 7. Configuration Maps (配置映射)
**关键数据结构**:
```python
module_to_stage_map    # 模块ID -> Stage ID 的映射
stage_to_rank_map      # Stage ID -> Rank列表 的映射
stage_to_depth_map     # Stage ID -> warmup深度 的映射
```

---

## 📖 关键配置文件详解

### 1. YAML配置文件 (例: bert_4vpipe.yml)
```yaml
directory: 'bert'                    # 模型代码目录
module: 'vgpus=4'                    # 模块名称（对应vgpus=4文件夹）
partition: 'vgpus=4/vpipe.json'      # 分区配置文件
config_file: 'vgpus=4/mp_conf.json'  # 模型配置文件

machines: ['localhost:0', 'localhost:1', 'localhost:2', 'localhost:3']
# 格式: IP地址:GPU_ID

sync_mode: 'asp'                     # 同步模式: asp(异步) / bsp(同步)
distributed_backend: 'gloo'          # 通信后端: gloo / nccl

batch_size: 16                       # 全局batch大小
learning_rate: 0.03                  # 学习率
epochs: 2                            # 训练轮数
```

### 2. 分区配置文件 (vpipe.json)
```json
{
    "partition": [12, 13, 13, 11],      // 每个stage包含的层数
    "recompute_ratio": [0.3, 0, 0, 0]   // 每个stage的重计算比例
}
```
**解释**:
- `partition`: 49层BERT被分成4个stage，分别包含12/13/13/11层
- `recompute_ratio`: 第一个stage重计算30%的层（最后的层），其他stage不重计算

---

## 🎯 推荐学习路径

### 阶段一: 理解整体流程 (2-3天)

#### 1. 从启动到运行的完整链路
按照以下顺序阅读代码：

**Step 1: driver.py (启动器)**
- 关注 `main` 函数的执行流程
- 理解如何解析YAML配置
- 观察如何构建分布式启动命令

**Step 2: main_with_runtime.py (训练主程序)**
- 从 `main()` 函数开始
- 重点看 `stage()` 函数：模型的定义和分区
- 理解 `train()` 函数：训练循环逻辑

**Step 3: runtime.py (运行时核心)**
- 先看 `StageRuntime.__init__()`: 初始化逻辑
- 再看 `train()` 方法：启动训练
- 最后看 `run_training_iteration()`: 一次迭代的完整流程

**学习建议**:
```bash
# 在代码中添加打印语句，观察执行流程
python driver.py --config_file configs/bert_4vpipe.yml
```

### 阶段二: 深入核心机制 (3-4天)

#### 2. 流水线调度机制
**重点文件**: `runtime.py` 的 `run_training_iteration()`

关键方法：
```python
receive_tensors_forward()    # 接收上游激活值
run_forward()                # 执行前向传播
send_tensors_forward()       # 发送激活值到下游

receive_tensors_backward()   # 接收下游梯度
run_backward()               # 执行反向传播
send_tensors_backward()      # 发送梯度到上游
```

**学习任务**:
- 绘制4个stage的时间线图，标注每个stage在每个时刻的操作
- 理解warmup阶段和稳定阶段的区别

#### 3. 通信机制
**重点文件**: `communication.py`

核心概念：
- **点对点通信**: `send()` / `recv()` 用于跨节点stage间通信
- **广播通信**: 用于同一stage内的数据并行
- **异步线程**: `start_helper_threads()` 启动后台通信线程

**学习任务**:
```python
# 理解 CommunicationHandler 的这些方法
register_tensor()           # 注册需要通信的张量
wait()                      # 等待通信完成
send() / recv()             # 发送/接收张量
```

#### 4. 权重存储机制
**重点文件**: `optimizer.py` 的 `OptimizerWithWeightStashing`

**学习任务**:
- 理解为什么需要weight stashing（异步训练中梯度对应的权重版本问题）
- 追踪 `step()` 方法：何时更新权重版本
- 理解 `load_old_params()` 和 `load_new_params()`: 版本切换逻辑

### 阶段三: 模型实现细节 (2-3天)

#### 5. BERT模型切分
**重点文件**: `bert/vpipe.py`

核心类：
```python
class Bert:
    generate_layer_blocks()    # 将BERT代码解析为block
    generate_stage()           # 根据partition生成stage代码

class Stage(torch.nn.Module):
    forward()                  # stage的前向传播
    cp_forward()              # 带checkpoint的前向传播（用于重计算）
```

**学习任务**:
- 理解BERT是如何被切分的
- 查看生成的stage代码（通过打印 `self.no_cp` 和 `self.cp`）
- 理解重计算如何节省内存

#### 6. 数据加载
**重点**: `main_with_runtime.py` 中的数据加载逻辑

```python
class BertPretrainingDataset    # BERT预训练数据集
stage_to_dataloader()           # 为每个stage创建dataloader
```

**关键点**:
- 只有第一个stage需要dataloader
- 其他stage通过通信接收激活值

### 阶段四: 实验与调优 (3-5天)

#### 7. 运行实验
```bash
# 1. 运行VPipe (4 GPU)
cd runtime
python driver.py --config_file configs/bert_4vpipe.yml

# 2. 对比GPipe
python driver.py --config_file configs/bert_4gpipe.yml

# 3. 对比PipeDream
python driver.py --config_file configs/bert_4pipedream.yml
```

#### 8. 性能分析
**查看日志**:
```bash
# 输出在 log_directory 指定的目录
cat output/<timestamp>/output.log.0
```

**关键指标**:
- Throughput (samples/sec): 吞吐量
- Time per iteration: 每次迭代时间
- Communication time: 通信时间占比

#### 9. 参数调优实验
尝试修改以下参数观察效果：

**batch_size**:
```yaml
batch_size: 8/16/32  # 观察吞吐量变化
```

**partition**:
```json
// 尝试不同的层分配方案
{"partition": [10, 15, 15, 9]}  // 不均匀分配
{"partition": [12, 13, 13, 11]} // 原始配置
```

**recompute_ratio**:
```json
// 尝试不同的重计算比例
{"recompute_ratio": [0.0, 0, 0, 0]}  // 不重计算
{"recompute_ratio": [0.5, 0, 0, 0]}  // 50%重计算
```

---

## 🔬 深入研究建议

### 1. 代码插桩调试
在关键位置添加日志：
```python
# 在 runtime.py 中
def receive_tensors_forward(self):
    print(f"[Rank {self.rank}] Receiving forward tensors, minibatch_id={self.forward_minibatch_id}")
    # ... 原有代码
```

### 2. 可视化时间线
修改 `runtime_utilities.py` 记录每个操作的时间戳：
```python
self.forward_stats.stats['start_time'] = time.time()
self.forward_stats.stats['end_time'] = time.time()
```

### 3. 对比实验
- **VPipe vs GPipe**: 观察异步调度的优势
- **VPipe vs PipeDream**: 观察weight stashing的必要性
- **启用/禁用重计算**: 观察内存-时间的权衡

### 4. 扩展阅读
论文推荐：
- GPipe (NIPS 2019): 同步流水线并行
- PipeDream (SOSP 2019): 异步流水线并行
- VPipe 论文（搜索相关文献）

---

## 💡 常见问题解答

### Q1: Stage、Rank、GPU的关系？
**A**: 
- GPU: 物理设备
- Rank: 进程ID，每个进程绑定一个GPU
- Stage: 模型分区，可以包含多个rank（数据并行）

例如：8 GPU，4 stages，每个stage 2个rank
```
Stage 0: Rank 0, 1  (GPU 0, 1)
Stage 1: Rank 2, 3  (GPU 2, 3)
Stage 2: Rank 4, 5  (GPU 4, 5)
Stage 3: Rank 6, 7  (GPU 6, 7)
```

### Q2: 为什么需要 tensor_tags？
**A**: 在分布式通信中，需要用tag区分不同的张量。例如：
- tag=1: input0
- tag=2: input1
- tag=3: out5
确保发送方和接收方通信的是同一个张量。

### Q3: num_warmup_minibatches 的作用？
**A**: 流水线并行需要warmup阶段填满pipeline：
```
Time  Stage0  Stage1  Stage2  Stage3
  0     F0      -       -       -     
  1     F1     F0       -       -     (warmup)
  2     F2     F1      F0       -     (warmup)
  3     F3     F2      F1      F0     (稳定状态)
```
Stage 3的warmup深度为3。

### Q4: 如何理解 recompute_ratio=0.3？
**A**: 
- Stage有10个block，recompute_ratio=0.3
- 前7个block正常保存激活值
- 后3个block的激活值不保存，反向时重新计算
- 节省30%的激活值内存

### Q5: ASP vs BSP 模式区别？
**A**:
- **BSP** (Bulk Synchronous Parallel): 所有minibatch的梯度应用后才开始下一轮
- **ASP** (Asynchronous Stale Parameter): 每个minibatch独立应用梯度，允许使用"陈旧"的权重
- ASP速度更快但需要weight stashing保证正确性

